{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawler for Yelp Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# packages to scrap content from web pages\n",
    "import selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.firefox_binary import FirefoxBinary\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "\n",
    "# interpretation of html \n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### entities to crawl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_search_entities = [\"Chick-fil-A\", \"Chipotle Mexican Grill\", \"Burger King\", \"KFC\", \"McDonald’s\", \"Subway Restaurants\", \"Taco Bell\", \"Wendy’s\", \"Dunkin’\", \"Papa John’s Pizza\", \"Pizza+Hut\", \"Starbucks\", \"Applebee’s+Grill+%2B+Bar\", \"Olive Garden Italian Restaurant\", 'Shake shack'\n",
    "]\n",
    "\n",
    "d_places = {'New York': 'New+York%2C+NY%2C+Vereinigte+Staaten', 'Los Angeles': 'Los Angeles%2C CA', 'Chicago': 'Chicago%2C IL', 'Houston': 'Houston%2C TX', 'Phoenix': 'Phoenix%2C AZ', 'Philadelphia': 'Philadelphia%2C PA', 'San Antonio': 'San Antonio%2C TX', 'San Diego': 'San Diego%2C CA', 'Dallas': 'Dallas%2C TX', 'San Jose': 'San Jose%2C CA', 'Austin': 'Austin%2C TX', 'Jacksonville': 'Jacksonville%2C FL', 'Fort Worth': 'Fort Worth%2C TX', 'Columbus': 'Columbus%2C OH', 'San Francisco': 'San Francisco%2C CA',\n",
    "'Charlotte': 'Charlotte%2C NC', 'Indianapolis': 'Indianapolis%2C IN', 'Seattle': 'Seattle%2C WA', 'Denver': 'Denver%2C CO', 'Washington': 'Washington%2C DC', 'Boston': 'Boston%2C MA', 'El Paso': 'El Paso%2C TX', 'Detroit': 'Detroit%2C MI', 'Nashville': 'Nashville%2C TN', 'Portland': 'Portland%2C OR', 'Memphis': 'Memphis%2C TN', 'Oklahoma City': 'Oklahoma City%2C OK', \n",
    "'Las Vegas': 'Las Vegas%2C NV', 'Louisville': 'Louisville%2C KY', 'Baltimore': 'Baltimore%2C MD', 'Milwaukee': 'Milwaukee%2C WI', 'Albuquerque': 'Albuquerque%2C NM', 'Tucson': 'Tucson%2C AZ', 'Fresno': 'Fresno%2C CA', 'Mesa': 'Mesa%2C AZ', \n",
    "'Sacramento': 'Sacramento%2C CA', 'Atlanta': 'Atlanta%2C GA', 'Kansas City': 'Kansas City%2C MO', 'Colorado Springs': 'Colorado Springs%2C CO', 'Miami': 'Miami%2C FL', 'Raleigh': 'Raleigh%2C NC', 'Omaha': 'Omaha%2C NE', \n",
    "'Long Beach': 'Long Beach%2C CA', 'Virginia Beach': 'Virginia Beach%2C VA', 'Oakland': 'Oakland%2C CA', 'Minneapolis': 'Minneapolis%2C MN', 'Tulsa': 'Tulsa%2C OK', 'Tampa': 'Tampa%2C FL', 'New Orleans': 'New Orleans%2C LA'\n",
    "}\n",
    "\n",
    "URL_BODY = \"https://www.yelp.com/search?find_desc=$1&find_loc=$2\"\n",
    "RESULT_PATH = r\"./\"\n",
    "RESULT_FILE = r\"yelp_restaurants.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### init driver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#driver = webdriver.Chrome(r\"P:\\chromedriver_win32\\chromedriver.exe\")\n",
    "driver = webdriver.Chrome(r\"C:/chromedriver.exe\")\n",
    "\n",
    "driver.implicitly_wait(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to crawl links from restaurants from overview page\n",
    "**input:** name of restaurant like 'Subway' (**string**), location like 'Boston' (**string**) <br>\n",
    "**output:** list of links to all found items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from selenium.common.exceptions import StaleElementReferenceException, ElementClickInterceptedException\n",
    "\n",
    "def get_items(item_name, place):\n",
    "    l_links = []\n",
    "    driver.get(URL_BODY.replace(\"$1\", item_name).replace(\"$2\", place))\n",
    "    while True:\n",
    "        time.sleep(1.5)\n",
    "        soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "\n",
    "        items = soup.select('a[class=\"css-1422juy\"]')\n",
    "        items = list(filter(lambda x: x.has_attr('name') and\n",
    "                                      item_name.lower() in x['name'].lower(), items))\n",
    "        if len(items) == 0: break\n",
    "\n",
    "        new_links = list(map(lambda x : x['href'], items))\n",
    "        l_links += new_links\n",
    "\n",
    "        try:\n",
    "            next_page = driver.find_element_by_css_selector('a[class*=\"next-link navigation-button__09f24__m9qRz css-1pxws0l\"]')\n",
    "        except NoSuchElementException or ElementClickInterceptedException or StaleElementReferenceException:\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            next_page.click()\n",
    "        except NoSuchElementException or ElementClickInterceptedException or StaleElementReferenceException:\n",
    "            break\n",
    "\n",
    "    return list(set(l_links))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing (not mandatory to execute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/biz/mcdonalds-houston-4?osq=McDonald%27s', '/biz/mcdonalds-houston-158?osq=McDonald%27s', '/biz/mcdonalds-houston-25?osq=McDonald%27s', '/biz/mcdonalds-houston-147?osq=McDonald%27s', '/biz/mcdonalds-houston-44?osq=McDonald%27s']\n"
     ]
    }
   ],
   "source": [
    "restaurants = get_items(\"McDonald’s\", 'Houston')\n",
    "print(restaurants)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to crawl reviews to a single item\n",
    "**input:** link to a certain restaurant/ item (**string**) (provided by function 'get_items()' above) <br>\n",
    "**output:** list of lists of rating, comment and date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_reviews(link):\n",
    "    driver.get(\"https://www.yelp.com%s&sort_by=date_desc\" % link)\n",
    "\n",
    "    l_ratings = []\n",
    "    l_comments = []\n",
    "    l_dates = []\n",
    "\n",
    "    while True:\n",
    "        time.sleep(1.5)\n",
    "        soup = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "        section_ratings = soup.select('section[class=\"margin-t4__09f24__G0VVf padding-t4__09f24__Y6aGL border--top__09f24__exYYb border-color--default__09f24__NPAKY\"]')\n",
    "\n",
    "        if not section_ratings:\n",
    "            print('no selection_ratings')\n",
    "            return [l_ratings, l_dates, l_comments]\n",
    "\n",
    "        # print(f'Comment page {page}, length of section ratings: {len(section_ratings)}')\n",
    "        for i in range(len(section_ratings)):\n",
    "            soup_ratings = BeautifulSoup(str(section_ratings[i]), features=\"html.parser\")\n",
    "            if not soup_ratings:\n",
    "                continue\n",
    "\n",
    "            ratings = soup_ratings.select('div[class=\"review__09f24__oHr9V border-color--default__09f24__NPAKY\"]')\n",
    "            if not ratings:\n",
    "                continue\n",
    "            # print(f'length of ratings: {len(ratings)}')\n",
    "            for j in range(len(ratings)):\n",
    "                # print(ratings[i])\n",
    "                soup_detail = BeautifulSoup(str(ratings[j]), features=\"html.parser\")\n",
    "\n",
    "                if soup_detail is None:\n",
    "                    continue\n",
    "\n",
    "                rating = soup_detail.select_one('div[class*=\"i-stars__09f24__foihJ\"]')\n",
    "                rating = rating['aria-label'] if rating else ''\n",
    "\n",
    "                comment = soup_detail.select_one('span[class*=\"raw__09f24__T4Ezm\"]')\n",
    "                comment = comment.text if comment else ''\n",
    "\n",
    "                date = soup_detail.select_one('span[class*=\"css-1e4fdj9\"]')\n",
    "                date = date.text if date else ''\n",
    "\n",
    "                l_ratings.append(str(rating))\n",
    "                l_comments.append(str(comment))\n",
    "                l_dates.append(str(date))\n",
    "\n",
    "        try:\n",
    "            next_comment_page = driver.find_element_by_css_selector('a[class*=\"next-link navigation-button__09f24__m9qRz\"]')\n",
    "        except NoSuchElementException or ElementClickInterceptedException or StaleElementReferenceException:\n",
    "            break\n",
    "        try:\n",
    "            next_comment_page.click()\n",
    "        except NoSuchElementException or ElementClickInterceptedException or StaleElementReferenceException:\n",
    "            break\n",
    "    \n",
    "    return [l_ratings, l_dates, l_comments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I mean...does anyone actually look at reviews of fast food places on Yelp? I\"m not sure, but this is the happiest crew I've ever seen. They deserved a shout.This McD's is brand spankin' new. The interior is bright and modern. Around 10am on a Saturday, it was buzzing. I interacted with 3-4 crew members, all of whom must have mixed Red Bull in their coffee that morning and were exceptionally perky and happy. Greeted when I entered, pleasant to order, order brought to the table. Food hot and tasty as usual (biscuit sandwich with bacon - mmm).I'm still going to knock them for limiting their breakfast hours to 10am and for their merely passable offerings of \"healthy\" foods, but salads, oatmeal, and apple slices are on the menu to their credit. We'll see if it keeps up the excellence.\n"
     ]
    }
   ],
   "source": [
    "test_review = handle_reviews(restaurants[1])\n",
    "print(test_review[2][-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to read already handled links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "def get_list_of_handled_links():\n",
    "    if not os.path.exists(RESULT_PATH + RESULT_FILE):\n",
    "        # df_temp = pd.DataFrame()\n",
    "        # df_temp.to_csv(columns=['link', 'restaurant', 'place', 'rating', 'date', 'comment'])\n",
    "        return None\n",
    "    df = pd.read_csv(RESULT_FILE, header=0, encoding = \"ISO-8859-1\")\n",
    "    if \"link\" in df:\n",
    "        return df[\"link\"].tolist()\n",
    "    else:\n",
    "        return None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Start crawling Chick-fil-A lists in New York\n",
      "Now, crawl detailed Chick-fil-A information in New York\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [00:30<02:03, 30.99s/it]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "handled_links = get_list_of_handled_links()\n",
    "print(f'Handled reviews length: {len(handled_links)}') if handled_links else print(handled_links)\n",
    "\n",
    "FINISHED_JSON_FILE = 'finished_cities_and_restaurants.json'\n",
    "\n",
    "for item in l_search_entities:\n",
    "    for place in d_places:\n",
    "        if not os.path.exists(FINISHED_JSON_FILE):\n",
    "            with open(FINISHED_JSON_FILE, 'w') as f:\n",
    "                json.dump({}, f)\n",
    "                f.close()\n",
    "        with open(FINISHED_JSON_FILE, 'r') as f:\n",
    "            finished: dict = json.load(f)\n",
    "            f.close()\n",
    "        if place in finished and item in finished[place]:\n",
    "            print(f'Already finished crawling {item} in {place}, passed.')\n",
    "            continue\n",
    "\n",
    "        l_link = []\n",
    "        l_restaurant = []\n",
    "        l_place = []\n",
    "        l_rating = []\n",
    "        l_date = []\n",
    "        l_text = []\n",
    "\n",
    "        # first, get all restaurant links\n",
    "        print(f'Start crawling {item} lists in {place}')\n",
    "        links = get_items(item, d_places[place])\n",
    "\n",
    "        if handled_links:\n",
    "            links_filtered = list(filter(lambda x: x not in handled_links, links))\n",
    "        else:\n",
    "            links_filtered = list(links)\n",
    "        print(f'Now, crawl detailed {item} information in {place}')\n",
    "        for i in tqdm(range(len(links_filtered))):\n",
    "            try:\n",
    "                # then, get (rating, comments, date)\n",
    "                l_results = handle_reviews(links_filtered[i])\n",
    "            except IndexError:\n",
    "                continue\n",
    "            \n",
    "            l_link += [links_filtered[i]] * len(l_results[0])\n",
    "            l_restaurant += [item] * len(l_results[0])\n",
    "            l_place += [place] * len(l_results[0])\n",
    "            l_rating += l_results[0]\n",
    "            l_date += l_results[1]\n",
    "            l_text += l_results[2]\n",
    "                \n",
    "\n",
    "        print(\"%s - %s, %d restaurants (%d already handled)\"\n",
    "              % (item, place, len(links), len(links) - len(links_filtered)))\n",
    "\n",
    "        df_grid = pd.DataFrame()\n",
    "        df_grid[\"link\"] = l_link\n",
    "        df_grid[\"restaurant\"] = l_restaurant\n",
    "        df_grid[\"place\"] = l_place\n",
    "        df_grid[\"rating\"] = l_rating\n",
    "        df_grid[\"date\"] = l_date\n",
    "        df_grid[\"comment\"] = l_text\n",
    "        df_grid.to_csv(RESULT_PATH + RESULT_FILE, mode='a')\n",
    "\n",
    "        finished[place] = finished.get(place, list())\n",
    "        finished[place].add(item)\n",
    "        with open(FINISHED_JSON_FILE, 'w') as f:\n",
    "            json.dump(finished, f)\n",
    "            f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Building DataFrame of crawled data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}